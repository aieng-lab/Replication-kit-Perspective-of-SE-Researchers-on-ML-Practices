{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get initial number and data from raw codes and subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read subjects after merging surveys and papers (excels)\n",
    "df_papers_raw_codes_p1 = pd.read_excel('../Data/Initial_tagging_subjects/initial_papers_first_round.xlsx')\n",
    "df_papers_raw_codes_p2 = pd.read_excel('../Data/Initial_tagging_subjects/initial_papers_second_round.xlsx')\n",
    "df_surveys_raw_codes = pd.read_excel('../Data/Initial_tagging_subjects/initial_and_merged_surveys.xlsx')\n",
    "df_interviews_raw_codes = pd.read_excel('../Data/Initial_tagging_subjects/initial_codes_interviews.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['List of papers', 'Practices',\n",
       "        'Location in the article (Page, section)',\n",
       "        'ML Practice Input (Data, models,..)',\n",
       "        'ML Practice What/How (technique/method,..)',\n",
       "        'ML Practice purpose (if it is mentioned)', 'ML pipeline Stage',\n",
       "        'Software engineering task', 'Unnamed: 8', 'removed?'],\n",
       "       dtype='object'),\n",
       " (682, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers_raw_codes_p1.columns, df_papers_raw_codes_p1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['List of papers', 'Practices',\n",
       "        'Location in the article (Page, section)', 'Unnamed: 3',\n",
       "        'ML Practice Input (Data, models,..)',\n",
       "        'ML Practice What/How (technique/method,..)',\n",
       "        'ML Practice purpose (if it is mentioned)', 'ML pipeline Stage',\n",
       "        'Software engineering task', 'removed?'],\n",
       "       dtype='object'),\n",
       " (699, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers_raw_codes_p2.columns, df_papers_raw_codes_p2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['List of papers', 'Practices',\n",
       "        'Location in the article (Page, section)',\n",
       "        'ML Practice Input (Data, models,..)',\n",
       "        'ML Practice What/How (technique/method,..)',\n",
       "        'ML Practice purpose (if it is mentioned)', 'ML pipeline Stage',\n",
       "        'Software engineering task', 'Unnamed: 8', 'removed?', 'Unnamed: 3'],\n",
       "       dtype='object'),\n",
       " (1381, 11))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STack paperr 1 and 2\n",
    "df_papers_raw_codes = pd.concat([df_papers_raw_codes_p1, df_papers_raw_codes_p2], ignore_index=True)\n",
    "df_papers_raw_codes.columns, df_papers_raw_codes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get variables from papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>List of papers</th>\n",
       "      <th>Practices</th>\n",
       "      <th>Location in the article (Page, section)</th>\n",
       "      <th>ML Practice Input (Data, models,..)</th>\n",
       "      <th>ML Practice What/How (technique/method,..)</th>\n",
       "      <th>ML Practice purpose (if it is mentioned)</th>\n",
       "      <th>ML pipeline Stage</th>\n",
       "      <th>Software engineering task</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>removed?</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>10.1109 ICSE.2019.00086</td>\n",
       "      <td>2, 9</td>\n",
       "      <td>page 785, section III</td>\n",
       "      <td>-</td>\n",
       "      <td>use a model ( Bi-GRU)</td>\n",
       "      <td>to model the naturalness of code statements</td>\n",
       "      <td>Model Training</td>\n",
       "      <td>Code representation/embedding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>10.1109 ICSE.2019.00086</td>\n",
       "      <td>1</td>\n",
       "      <td>page 784, Section III A</td>\n",
       "      <td>source code fragements</td>\n",
       "      <td>build abstract syntax tree (AST)</td>\n",
       "      <td>transform data in a usable structure</td>\n",
       "      <td>Feature Engineering</td>\n",
       "      <td>Code summarization</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>10.1109 ICSE.2019.00086</td>\n",
       "      <td>10</td>\n",
       "      <td>page 785, section III A</td>\n",
       "      <td>abstract sintex tree</td>\n",
       "      <td>split AST by sentences</td>\n",
       "      <td>-</td>\n",
       "      <td>Data Cleaning</td>\n",
       "      <td>Code representation/embedding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>10.1109 ICSE.2019.00086</td>\n",
       "      <td>11</td>\n",
       "      <td>page 787, section III B.2</td>\n",
       "      <td>data (code fragments)</td>\n",
       "      <td>encode multiple code fragments simultaneously ...</td>\n",
       "      <td>For improving the training efficiency</td>\n",
       "      <td>Model Training</td>\n",
       "      <td>Code representation/embedding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>10.1109 ICSE.2019.00086</td>\n",
       "      <td>12</td>\n",
       "      <td>page 787, section III C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Use a bidirectional GRU [34], where the hidden...</td>\n",
       "      <td>to further enhance the capability of the recur...</td>\n",
       "      <td>Model Training</td>\n",
       "      <td>Code representation/embedding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>10.1109 ICSE.2019.00086</td>\n",
       "      <td>3, 13</td>\n",
       "      <td>page 788, section V A</td>\n",
       "      <td>-</td>\n",
       "      <td>use two public dataset that are usually uased ...</td>\n",
       "      <td>to evaluate their approach</td>\n",
       "      <td>Model Evaluation</td>\n",
       "      <td>Code representation/embedding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>10.1109 ICSE.2019.00086</td>\n",
       "      <td>14</td>\n",
       "      <td>page 788, section V B</td>\n",
       "      <td>whole dataset</td>\n",
       "      <td>randomly divide it into three parts,  60%, 20%...</td>\n",
       "      <td>split data based on their purpose</td>\n",
       "      <td>Data Cleaning</td>\n",
       "      <td>Code representation/embedding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>10.1109 ICSE.2019.00086</td>\n",
       "      <td>8</td>\n",
       "      <td>page 789, Section V C.1</td>\n",
       "      <td>source code</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>evaluate model</td>\n",
       "      <td>Model Evaluation</td>\n",
       "      <td>Code summarization</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>10.1109 ICSE.2019.00086</td>\n",
       "      <td>15</td>\n",
       "      <td>page 789, section V C.2</td>\n",
       "      <td>model + data</td>\n",
       "      <td>Recall, Precision, F1</td>\n",
       "      <td>to evaluate the model performance</td>\n",
       "      <td>Model Evaluation</td>\n",
       "      <td>Code representation/embedding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>10.1109 ICSE.2019.00086</td>\n",
       "      <td>4, 5, 6, 7</td>\n",
       "      <td>page 789, section V C</td>\n",
       "      <td>model+ data</td>\n",
       "      <td>state-of-the-art baselines</td>\n",
       "      <td>to evaluate model performance</td>\n",
       "      <td>Model Evaluation</td>\n",
       "      <td>Code representation/embedding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              List of papers   Practices  \\\n",
       "136  10.1109 ICSE.2019.00086        2, 9   \n",
       "137  10.1109 ICSE.2019.00086           1   \n",
       "138  10.1109 ICSE.2019.00086          10   \n",
       "139  10.1109 ICSE.2019.00086          11   \n",
       "140  10.1109 ICSE.2019.00086          12   \n",
       "141  10.1109 ICSE.2019.00086       3, 13   \n",
       "142  10.1109 ICSE.2019.00086          14   \n",
       "143  10.1109 ICSE.2019.00086           8   \n",
       "144  10.1109 ICSE.2019.00086          15   \n",
       "145  10.1109 ICSE.2019.00086  4, 5, 6, 7   \n",
       "\n",
       "    Location in the article (Page, section)  \\\n",
       "136                   page 785, section III   \n",
       "137                 page 784, Section III A   \n",
       "138                 page 785, section III A   \n",
       "139               page 787, section III B.2   \n",
       "140                 page 787, section III C   \n",
       "141                   page 788, section V A   \n",
       "142                   page 788, section V B   \n",
       "143                 page 789, Section V C.1   \n",
       "144                 page 789, section V C.2   \n",
       "145                   page 789, section V C   \n",
       "\n",
       "    ML Practice Input (Data, models,..)  \\\n",
       "136                                   -   \n",
       "137              source code fragements   \n",
       "138                abstract sintex tree   \n",
       "139               data (code fragments)   \n",
       "140                                 NaN   \n",
       "141                                   -   \n",
       "142                       whole dataset   \n",
       "143                         source code   \n",
       "144                        model + data   \n",
       "145                         model+ data   \n",
       "\n",
       "            ML Practice What/How (technique/method,..)  \\\n",
       "136                              use a model ( Bi-GRU)   \n",
       "137                   build abstract syntax tree (AST)   \n",
       "138                             split AST by sentences   \n",
       "139  encode multiple code fragments simultaneously ...   \n",
       "140  Use a bidirectional GRU [34], where the hidden...   \n",
       "141  use two public dataset that are usually uased ...   \n",
       "142  randomly divide it into three parts,  60%, 20%...   \n",
       "143                                           Accuracy   \n",
       "144                              Recall, Precision, F1   \n",
       "145                         state-of-the-art baselines   \n",
       "\n",
       "              ML Practice purpose (if it is mentioned)    ML pipeline Stage  \\\n",
       "136        to model the naturalness of code statements       Model Training   \n",
       "137               transform data in a usable structure  Feature Engineering   \n",
       "138                                                  -        Data Cleaning   \n",
       "139              For improving the training efficiency       Model Training   \n",
       "140  to further enhance the capability of the recur...       Model Training   \n",
       "141                         to evaluate their approach     Model Evaluation   \n",
       "142                  split data based on their purpose        Data Cleaning   \n",
       "143                                     evaluate model     Model Evaluation   \n",
       "144                  to evaluate the model performance     Model Evaluation   \n",
       "145                      to evaluate model performance     Model Evaluation   \n",
       "\n",
       "         Software engineering task  Unnamed: 8 removed?  Unnamed: 3  \n",
       "136  Code representation/embedding         NaN      NaN         NaN  \n",
       "137             Code summarization         NaN      NaN         NaN  \n",
       "138  Code representation/embedding         NaN      NaN         NaN  \n",
       "139  Code representation/embedding         NaN      NaN         NaN  \n",
       "140  Code representation/embedding         NaN      NaN         NaN  \n",
       "141  Code representation/embedding         NaN      NaN         NaN  \n",
       "142  Code representation/embedding         NaN      NaN         NaN  \n",
       "143             Code summarization         NaN      NaN         NaN  \n",
       "144  Code representation/embedding         NaN      NaN         NaN  \n",
       "145  Code representation/embedding         NaN      NaN         NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers_raw_codes[df_papers_raw_codes['List of papers'] == '10.1109 ICSE.2019.00086']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 'Var1.1 - Input (of  the practice)' of df_raw_codes\n",
    "input_p = np.unique(df_papers_raw_codes['ML Practice Input (Data, models,..)'].dropna().values)\n",
    "# get  'ML Practice What/How (technique/method,..)' of df_raw_codes\n",
    "technique_p = np.unique(df_papers_raw_codes['ML Practice What/How (technique/method,..)'].dropna().values)\n",
    "# get  'Var1.3 - Purpose/Output' of df_raw_codes\n",
    "purpose_p = np.unique(df_papers_raw_codes['ML Practice purpose (if it is mentioned)'].dropna().values)\n",
    "# get  'Var2 - ML pipeline stage(s)' of df_raw_codes\n",
    "stage_p = np.unique(df_papers_raw_codes['ML pipeline Stage'].dropna().values)\n",
    "# get  'Var3 - SE task' of df_raw_codes\n",
    "task_p = np.unique(df_papers_raw_codes['Software engineering task'].dropna().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raw codes (532,)\n",
      "Technique raw codes (1295,)\n",
      "Purpose raw codes (959,)\n",
      "Stage raw codes (11,)\n",
      "Task raw codes (91,)\n"
     ]
    }
   ],
   "source": [
    "# get the unique number per variable \n",
    "print('Input raw codes', input_p.shape)\n",
    "print('Technique raw codes', technique_p.shape)\n",
    "print('Purpose raw codes', purpose_p.shape)\n",
    "print('Stage raw codes', stage_p.shape)\n",
    "print('Task raw codes', task_p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get variables from interviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Interview', 'Line (or range*)', 'Var1.1 - Input (of  the practice)',\n",
       "       'Var1.2 - Technique', 'Var1.3 - Purpose/Output',\n",
       "       'Var2 - ML pipeline stage(s)', 'Var3 - SE task',\n",
       "       'Var4 - Quality attributes', 'Var5 - ML Principles',\n",
       "       'Var6 - ML Challenges',\n",
       "       'Var7 - Challenges to ensure quality attributes',\n",
       "       'Var 8 - Reviewer´s  perspective', 'Var9 - Educator's perspective',\n",
       "       'Unnamed: 13'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interviews_raw_codes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 'Var1.1 - Input (of  the practice)' of df_raw_codes\n",
    "input_i = np.unique(df_interviews_raw_codes['Var1.1 - Input (of  the practice)'].dropna().values)\n",
    "# get  'ML Practice What/How (technique/method,..)' of df_raw_codes\n",
    "technique_i = np.unique(df_interviews_raw_codes['Var1.2 - Technique'].dropna().values)\n",
    "# get  'Var1.3 - Purpose/Output' of df_raw_codes\n",
    "purpose_i = np.unique(df_interviews_raw_codes['Var1.3 - Purpose/Output'].dropna().values)\n",
    "# get  'Var2 - ML pipeline stage(s)' of df_raw_codes\n",
    "stage_i = np.unique(df_interviews_raw_codes['Var2 - ML pipeline stage(s)'].dropna().values)\n",
    "# get  'Var3 - SE task' of df_raw_codes\n",
    "task_i = np.unique(df_interviews_raw_codes['Var3 - SE task'].dropna().values)\n",
    "# get 'Var4 - Quality attributes' of df_raw_codes\n",
    "quality_i = np.unique(df_interviews_raw_codes['Var4 - Quality attributes'].dropna().values)\n",
    "# get 'Var5 - ML Principles' of df_raw_codes\n",
    "principles_i = np.unique(df_interviews_raw_codes['Var5 - ML Principles'].dropna().values)\n",
    "# get 'Var6 - ML Challenges' of df_raw_codes\n",
    "challengesML_i = np.unique(df_interviews_raw_codes['Var6 - ML Challenges'].dropna().values)\n",
    "# get 'Var7 - Challenges to ensure quality attributes' of df_raw_codes\n",
    "challenges_quality_i = np.unique(df_interviews_raw_codes['Var7 - Challenges to ensure quality attributes'].dropna().values)\n",
    "challenges_i = set(list(challengesML_i) + list(challenges_quality_i))\n",
    "# get 'Var 8 - Reviewer´s  perspective' of df_raw_codes\n",
    "reviewers_i = np.unique(df_interviews_raw_codes['Var 8 - Reviewer´s  perspective'].dropna().values)\n",
    "# get 'Var9 - Educator's perspective' of df_raw_codes\n",
    "educator_i = np.unique(df_interviews_raw_codes['Var9 - Educator\\'s perspective'].dropna().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raw codes (70,)\n",
      "Technique raw codes (442,)\n",
      "Purpose raw codes (315,)\n",
      "Stage raw codes (12,)\n",
      "Task raw codes (37,)\n",
      "Quality raw codes (36,)\n",
      "Principles raw codes (3,)\n",
      "Challenges raw codes 89\n",
      "Reviewers raw codes (71,)\n",
      "Educator raw codes (42,)\n"
     ]
    }
   ],
   "source": [
    "# get the unique number per variable \n",
    "print('Input raw codes', input_i.shape)\n",
    "print('Technique raw codes', technique_i.shape)\n",
    "print('Purpose raw codes', purpose_i.shape)\n",
    "print('Stage raw codes', stage_i.shape)\n",
    "print('Task raw codes', task_i.shape)\n",
    "print('Quality raw codes', quality_i.shape)\n",
    "print('Principles raw codes', principles_i.shape)\n",
    "print('Challenges raw codes', len(challenges_i))\n",
    "print('Reviewers raw codes', reviewers_i.shape)\n",
    "print('Educator raw codes', educator_i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get variables from surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Tagger/merge', 'Survey', 'ID - Survey',\n",
       "       'ML Practice Input (Data, models,..)',\n",
       "       'ML Practice What/How (technique/method,..)',\n",
       "       'ML Practice output AND/OR purpose (if it is mentioned)',\n",
       "       'ML pipeline Stage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_surveys_raw_codes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 'Var1.1 - Input (of  the practice)' of df_raw_codes\n",
    "input_s = np.unique(df_surveys_raw_codes['ML Practice Input (Data, models,..)'].dropna().values)\n",
    "# get  'ML Practice What/How (technique/method,..)' of df_raw_codes\n",
    "technique_s = np.unique(df_surveys_raw_codes['ML Practice What/How (technique/method,..)'].dropna().values)\n",
    "# get  'Var1.3 - Purpose/Output' of df_raw_codes\n",
    "purpose_s = np.unique(df_surveys_raw_codes['ML Practice output AND/OR purpose (if it is mentioned)'].dropna().values)\n",
    "# get  'Var2 - ML pipeline stage(s)' of df_raw_codes\n",
    "stage_s = np.unique(df_surveys_raw_codes['ML pipeline Stage'].dropna().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raw codes (8,)\n",
      "Technique raw codes (162,)\n",
      "Purpose raw codes (43,)\n",
      "Stage raw codes (9,)\n"
     ]
    }
   ],
   "source": [
    "# get the unique number per variable \n",
    "print('Input raw codes', input_s.shape)\n",
    "print('Technique raw codes', technique_s.shape)\n",
    "print('Purpose raw codes', purpose_s.shape)\n",
    "print('Stage raw codes', stage_s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify in a single variable per variable the codes of the three sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unify in a single variable per variable the codes of the three sources\n",
    "input = set(list(input_p) + list(input_i) + list(input_s))\n",
    "input = {x for x in input if pd.notna(x)}\n",
    "technique = set(list(technique_p) + list(technique_i) + list(technique_s))\n",
    "technique = {x for x in technique if pd.notna(x)}\n",
    "purpose = set(list(purpose_p) + list(purpose_i) + list(purpose_s))\n",
    "purpose = {x for x in purpose if pd.notna(x)}\n",
    "stage = set(list(stage_p) + list(stage_i) + list(stage_s))\n",
    "stage = {x for x in stage if pd.notna(x)}\n",
    "task = set(list(task_p) + list(task_i))\n",
    "task = {x for x in task if pd.notna(x)}\n",
    "quality = set(quality_i)\n",
    "quality = {x for x in quality if pd.notna(x)}\n",
    "principles = set(principles_i)\n",
    "principles = {x for x in principles if pd.notna(x)}\n",
    "challenges = set(challenges_i)\n",
    "challenges = {x for x in challenges if pd.notna(x)}\n",
    "reviewers = set(reviewers_i)\n",
    "reviewers = {x for x in reviewers if pd.notna(x)}\n",
    "educator = set(educator_i)\n",
    "educator = {x for x in educator if pd.notna(x)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raw codes 598\n",
      "Technique raw codes 1843\n",
      "Purpose raw codes 1293\n",
      "ML Stage raw codes 14\n",
      "SE Task raw codes 118\n",
      "Quality raw codes 36\n",
      "Principles raw codes 3\n",
      "Challenges raw codes 89\n",
      "Reviewers' perspective raw codes 71\n",
      "Educator's perspective raw codes 42\n"
     ]
    }
   ],
   "source": [
    "# get the numbers of unique codes in each variable\n",
    "print('Input raw codes', len(input))\n",
    "print('Technique raw codes', len(technique))\n",
    "print('Purpose raw codes', len(purpose))\n",
    "print('ML Stage raw codes', len(stage))\n",
    "print('SE Task raw codes', len(task))\n",
    "print('Quality raw codes', len(quality))\n",
    "print('Principles raw codes', len(principles))\n",
    "print('Challenges raw codes', len(challenges))\n",
    "print('Reviewers\\' perspective raw codes', len(reviewers))\n",
    "print('Educator\\'s perspective raw codes', len(educator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics of the lenght of the interviews lines\n",
    "### Note that the following files are not included, the same as the scripts for the interviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_surveys_lines = pd.read_excel('/survey_split_by_line_first_and_second_round.xlsx') # Surveys splitted by line/practice\n",
    "df_surveys_full_r = pd.read_excel('/Merged_rounds_surveys.xlsx') # Full surveys (not splitted by line/practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Unique ID', 'Date Taken', 'Time Taken', 'Time Spent',\n",
       "       'Q1 - \"What machine learning best practices have you used in your software engineering research papers, and how often did you apply those practices (Always, Frequent, Sometimes, Never)?',\n",
       "       'Filter out'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_surveys_lines.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unique ID', 'Email', 'Date Taken (America/Bogota)',\n",
       "       'Time Taken (America/Bogota)', 'Browser', 'OS', 'City', 'Country',\n",
       "       'Referrer', 'Time Spent',\n",
       "       'Q1 - \"What machine learning best practices have you used in your software engineering research papers, and how often did you apply those practices (Always, Frequent, Sometimes, Never)?',\n",
       "       'Filtered?'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_surveys_full_r.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_surveys_lines = df_surveys_lines[['ID', 'Unique ID','Q1 - \"What machine learning best practices have you used in your software engineering research papers, and how often did you apply those practices (Always, Frequent, Sometimes, Never)?',\n",
    "       'Filter out']].copy()\n",
    "df_surveys_lines.shape\n",
    "\n",
    "df_surveys_full_r = df_surveys_full_r[['Unique ID','Q1 - \"What machine learning best practices have you used in your software engineering research papers, and how often did you apply those practices (Always, Frequent, Sometimes, Never)?', 'Filtered?']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_surveys_lines['Unique ID'].nunique() # unique number of surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_surveys_full_r['Unique ID'].nunique() # unique number of surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223, 4)\n"
     ]
    }
   ],
   "source": [
    "df_surveys_lines.columns = ['ID', 'Unique ID','Q1', 'Filter out']\n",
    "df_surveys_full_r.columns = ['Unique ID', 'Q1', 'Filter out']\n",
    "# filter out rows that have a x in the Filter out column\n",
    "df_surveys_lines = df_surveys_lines[df_surveys_lines['Filter out'] != 'x'].copy()\n",
    "df_surveys_full_r = df_surveys_full_r[df_surveys_full_r['Filter out'] != 'x'].copy()\n",
    "\n",
    "print(df_surveys_lines.shape)\n",
    "# df_surveys_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_surveys_full_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_surveys_lines['Unique ID'].nunique() # unique number of CODED surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_surveys_full_r['Unique ID'].nunique() # unique number of CODED surveys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean lines of the surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_surveys_lines['Q1_adapted_nf'] = df_surveys_lines['Q1'].str.replace(r'\\s*\\[.*?\\]\\s*', ' ', regex=True) # Remove frequency\n",
    "df_surveys_lines['Q1_adapted_nf'] = df_surveys_lines['Q1_adapted_nf'].str.replace(r'\\d+', ' ', regex=True) # remove numbers\n",
    "df_surveys_lines['Q1_adapted_nf'] = df_surveys_lines['Q1_adapted_nf'].str.replace(r'[()\\.,!]', ' ', regex=True) # remove punctuation\n",
    "df_surveys_lines['Q1_adapted_nf'] = df_surveys_lines['Q1_adapted_nf'].str.replace(r'\\s+', ' ', regex=True).str.strip()  # Remove extra spaces\n",
    "\n",
    "df_surveys_lines['Q1_adapted_wf'] = df_surveys_lines['Q1'].str.replace(r'\\d+', ' ', regex=True) # remove numbers\n",
    "df_surveys_lines['Q1_adapted_wf'] = df_surveys_lines['Q1_adapted_wf'].str.replace(r'[()\\.,!]', ' ', regex=True) # remove punctuation\n",
    "df_surveys_lines['Q1_adapted_wf'] = df_surveys_lines['Q1_adapted_wf'].str.replace(r'\\s+', ' ', regex=True).str.strip()  # Remove extra "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats survey lines (practices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.9237668161435\n",
      "53.74887892376682\n"
     ]
    }
   ],
   "source": [
    "print(df_surveys_lines['Q1_adapted_nf'].str.len().mean()) # mean of the length of the answers (practices)  with no frequency, no numbers, no punctuation, no extra spaces\n",
    "print(df_surveys_lines['Q1_adapted_wf'].str.len().mean()) # mean of the length of the answers (practices) with frequency, no numbers, no punctuation, no extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_surveys_full_r['lenght'] = df_surveys_full_r['Q1'].str.len() # length of the answers (practices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats survey before breaking lines FULL SURVEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259.06382978723406"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_surveys_full_r['lenght'].mean() # Mean of characters for the answers (practices) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interviews duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_time_minutes = {#seconds per interview\n",
    "    'i1':2993,\n",
    "    'i2':2872,\n",
    "    'i3':2414,\n",
    "    'i4':2846,\n",
    "    'i5':3082,\n",
    "    'i6':3002,\n",
    "    'i7':2677,\n",
    "    'i8':2053,\n",
    "    'i9':3189,\n",
    "    'i10':2562,\n",
    "    'i11':3585,\n",
    "    'i12':3361,\n",
    "    'i13':3263,\n",
    "    'i14':2805\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.45714285714286"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(dict_time_minutes.values()))/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpaper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
